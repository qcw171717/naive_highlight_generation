{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_vec_dict = {}\n",
    "with open ('word_2_vec_dict.pkl', 'rb') as file:\n",
    "    word_2_vec_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "tokens = []\n",
    "with open ('test.txt') as file:\n",
    "    for line in file:\n",
    "        if line.strip() != '':\n",
    "            line.replace('!', '.')\n",
    "            line.replace('?', '.')\n",
    "            line.replace('...', '.')\n",
    "            sentences_lst = line.split('.')\n",
    "            for sentence in sentences_lst:\n",
    "                if sentence.strip() != '' and len(sentence.strip()) > 5:\n",
    "                    sentences.append(sentence.strip())\n",
    "                    tokens.extend(line.strip().split())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = []\n",
    "for token in tokens:\n",
    "    if token.replace(',', '') != '' :\n",
    "        clean_tokens.append(token.replace(',', ''))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "0.286217239461\n",
      "33\n",
      "0.299265865766\n",
      "32\n",
      "0.286640184091\n",
      "31\n",
      "0.261050939728\n",
      "30\n",
      "0.253266213076\n",
      "29\n",
      "0.209993796465\n",
      "28\n",
      "0.338888500073\n",
      "27\n",
      "0.306853455221\n",
      "26\n",
      "0.465430344237\n",
      "25\n",
      "0.292893340538\n",
      "24\n",
      "0.247223085825\n",
      "23\n",
      "0.265916505409\n",
      "22\n",
      "0.317377197333\n",
      "21\n",
      "0.311576742189\n",
      "20\n",
      "0.260884854902\n",
      "19\n",
      "0.294365931814\n",
      "18\n",
      "0.223615402769\n",
      "17\n",
      "0.268597445931\n",
      "16\n",
      "0.411856510672\n",
      "15\n",
      "0.184497590147\n",
      "14\n",
      "0.319854414137\n",
      "13\n",
      "0.301817569806\n",
      "12\n",
      "0.265732091574\n",
      "11\n",
      "0.262154991264\n",
      "10\n",
      "0.28092482783\n",
      "9\n",
      "0.255793822623\n",
      "8\n",
      "0.281361494286\n",
      "7\n",
      "0.331605732118\n",
      "6\n",
      "0.219393125659\n",
      "5\n",
      "0.285032546361\n",
      "4\n",
      "0.236405547904\n",
      "3\n",
      "0.34527022066\n",
      "2\n",
      "0.302405574865\n",
      "1\n",
      "0.222270784624\n",
      "0\n",
      "0.285857731078\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(clean_tokens)\n",
    "sentence_av_cost = []\n",
    "count = 0\n",
    "for sentence in sentences:\n",
    "    cost_for_sentence = 0\n",
    "    word_lst = sentence.split()\n",
    "    num_words = 0\n",
    "    for word in word_lst:\n",
    "        cost_for_word = 0\n",
    "        #add average cos distance for word\n",
    "        for token in clean_tokens:\n",
    "            try:\n",
    "                cost_for_word += spatial.distance.cosine(word_2_vec_dict[word.lower()], word_2_vec_dict[token.lower()])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        cost_for_word = cost_for_word / num_tokens\n",
    "        num_words += 1\n",
    "        cost_for_sentence += cost_for_word\n",
    "    count += 1\n",
    "    print(len(sentences) - count)\n",
    "    sentence_av_cost.append(cost_for_sentence/num_words)\n",
    "    print(cost_for_sentence/num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('test_random.pkl', 'wb') as file:\n",
    "    pickle.dump(sentence_av_cost, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum = min(sentence_av_cost)\n",
    "sentence_av_cost.index(minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum = max(sentence_av_cost[1:])\n",
    "sentence_av_cost.index(maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "sentence_av_cost = []\n",
    "with open ('test_random.pkl', 'rb') as file:\n",
    "    sentence_av_cost = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28621723946090194, 0.29926586576607955, 0.28664018409109748], [0.29926586576607955, 0.28664018409109748, 0.26105093972756294], [0.28664018409109748, 0.26105093972756294, 0.2532662130757305], [0.26105093972756294, 0.2532662130757305, 0.20999379646534508], [0.2532662130757305, 0.20999379646534508, 0.33888850007288346], [0.20999379646534508, 0.33888850007288346, 0.30685345522095314], [0.33888850007288346, 0.30685345522095314, 0.46543034423699553], [0.30685345522095314, 0.46543034423699553, 0.29289334053834609], [0.46543034423699553, 0.29289334053834609, 0.24722308582534025], [0.29289334053834609, 0.24722308582534025, 0.26591650540862638], [0.24722308582534025, 0.26591650540862638, 0.31737719733295972], [0.26591650540862638, 0.31737719733295972, 0.31157674218876741], [0.31737719733295972, 0.31157674218876741, 0.260884854902448], [0.31157674218876741, 0.260884854902448, 0.29436593181433496], [0.260884854902448, 0.29436593181433496, 0.22361540276867564], [0.29436593181433496, 0.22361540276867564, 0.26859744593101426], [0.22361540276867564, 0.26859744593101426, 0.41185651067235091], [0.26859744593101426, 0.41185651067235091, 0.18449759014733313], [0.41185651067235091, 0.18449759014733313, 0.31985441413663068], [0.18449759014733313, 0.31985441413663068, 0.30181756980645791], [0.31985441413663068, 0.30181756980645791, 0.26573209157386724], [0.30181756980645791, 0.26573209157386724, 0.26215499126401121], [0.26573209157386724, 0.26215499126401121, 0.28092482782987038], [0.26215499126401121, 0.28092482782987038, 0.2557938226228631], [0.28092482782987038, 0.2557938226228631, 0.28136149428603252], [0.2557938226228631, 0.28136149428603252, 0.3316057321182197], [0.28136149428603252, 0.3316057321182197, 0.21939312565899172], [0.3316057321182197, 0.21939312565899172, 0.2850325463607129], [0.21939312565899172, 0.2850325463607129, 0.2364055479037335], [0.2850325463607129, 0.2364055479037335, 0.3452702206603428], [0.2364055479037335, 0.3452702206603428, 0.30240557486493452], [0.3452702206603428, 0.30240557486493452, 0.22227078462389127], [0.30240557486493452, 0.22227078462389127, 0.28585773107795487]]\n"
     ]
    }
   ],
   "source": [
    "tri_lst = []\n",
    "for i in range(len(sentence_av_cost) - 2):\n",
    "    tri_lst.append(sentence_av_cost[i:i+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87212328931807903, 0.84695698958473997, 0.80095733689439086, 0.72431094926863859, 0.80214850961395912, 0.85573575175918171, 1.1111722995308322, 1.0651771399962948, 1.0055467706006818, 0.80603293177231272, 0.83051678856692646, 0.8948704449303535, 0.88983879442417524, 0.86682752890555048, 0.7788661894854586, 0.78657878051402497, 0.9040693593720408, 0.8649515467506983, 0.91620851495631461, 0.80616957409042167, 0.88740407551695588, 0.82970465264433635, 0.80881191066774882, 0.79887364171674469, 0.818080144738766, 0.86876104902711537, 0.83236035206324399, 0.83603140413792421, 0.74083121992343814, 0.86670831492478917, 0.8840813434290109, 0.86994658014916859, 0.81053409056678061]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tri_lst)):\n",
    "    tri_lst[i] = sum(tri_lst[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_start = tri_lst.index(min(tri_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is there ever a point where you thought that was the case?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[min_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, and you say you've heard it at all times from all sides\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[min_start + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've been hearing it, you know, all my life\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[min_start + 2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
